---
layout: post
title: '多层感知机 介绍'
subtitle: '多层感知机 介绍'
description: '深入介绍了 多层感知机'
date: 2019-02-16
lastmod: 2019-02-16
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# 多层感知机 介绍

## 1. 隐藏层

**多层感知机** 就是在 *单层神经网络* 添加了一个或多个 **隐藏层**，*隐藏层* 位于 *输入层* 与 *输出层* 之间。

![多层感知机](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/mlp.svg "多层感知机-神经网络表示")

个人认为：**神经网络中的每一层都是一个特征层，而每个神经元都代表了一个特征。**比如在 *线性回归* 和 *softmax回归* 中第一层为最原始的特征，也就是数据本身，通过对这些最原始的特征的计算，将其转换为更能反映事实的特征，也就是我们需要的结果——数据标签。而在 *多层感知机* 中添加的 *隐藏层* 可以看作是对特征进一步抽象，通常这么做可以使得模型关注的数据区域越来越大，也就能有越来越多新的特征被发现。

为了更好的说明，现在来举个例子，假设我现在需要利用 *多层感知机* 进行 *人脸识别*。读者可以先看下面一幅图，再读下面这段话。

假设在第一层计算中我们先使用一个 $2\times2$ 的 *核* 来观察一幅图像，将其特征提取出来，此时我们观察的是图像的细节部分，例如像素的颜色等，然后我们再对提取出来的特征利用一个 $5\times5$ 的 *核* 来观察，此时我们实际上观察的就是一个 $10\times10$ 的区域，此时我们能够观察的范围比第一层能够观察的范围更大，因此得出的特征也会更加合理，以此类推，我们在某一层中就可以观察足够大的范围，使得模型能够完成我们的目标，但这并不是说 *神经网络* 层数越多，模型就越准确。

![多层感知机-隐藏层](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/mlp-hidden-layler.png "多层感知机-隐藏层-示例")



## 2. 模型建立

现在给定一个小批量样本 $\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其中批量大小为 $n$，每个样本的输入个数为 $d$。假设 *多层感知机* 只有一个 *隐藏层*，其中隐藏单元个数为 $h$。记 *隐藏层* 的输出为 $\boldsymbol{H}$，则有 $\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为 *隐藏层* 和 *输出层* 均为全连接层，则可设 *隐藏层* 的 *权重参数* 和 *偏差参数* 分别为 $\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$ 和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，假设 *输出层* 的单元个数为 $q$，记 *输出层* 输出为 $\boldsymbol{O}$，则有 $\boldsymbol{O} \in \mathbb{R}^{n \times q}$， 则 *输出层* 的 *权重参数* 和 *偏差参数* 分别为 $\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$ 和 $\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    

此时每一层的计算公式为：



$$
\begin{split}\begin{aligned}
\boldsymbol{H} &= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\
\boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,
\end{aligned}\end{split}
$$


通过将上式联立，可得：



$$
\boldsymbol{O} = (\boldsymbol{X}\boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X}\boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h\boldsymbol{W}_o + \boldsymbol{b}_o
$$


仔细观察上式，我们发现尽管我们的 *多层感知机* 引入了 *隐藏层*，但是上式却依旧等价于一个 *单层神经网络*。可以看出，上述设计存在明显缺陷，因此无论我们添加多少隐藏层，它依旧等价于一个 *单层神经网络*。这是因为 **线性函数具有 *齐次性* 和 *可加性*，任意线性函数连接都可以等价于一个单一的线性函数**。因此 *线性神经网络* 没有办法解决非线性问题，那么接下来，我们就将 *线性神经网络* 转换为 *非线性神经网络*。



## 3. 激活函数

上面的问题的根源在于 *全连接层* 只是对数据做了一个 **仿射变换（affine transformation）**，多个 *仿射变换* 的叠加依旧是一个 *仿射变换*。为了解决这个问题，我们可以引入 **非线性变换**，这个 *非线性变换函数* 被称为 **激活函数（activation function）**。下面介绍集中常用的激活函数。

下表中的激活函数，只需要一个变量作为输入：

|    名称     |                    方程                    |                             导数                             |      值域      |
| :---------: | :----------------------------------------: | :----------------------------------------------------------: | :------------: |
| sigmoid函数 |         $f(x)=\frac{1}{1+exp(-x)}$         |                   $f'(x) = f(x)(1 - f(x))$                   |    $(0, 1)$    |
|  ReLU函数   |             $f(x) = max(0, x)$             | $$f'(x) = \begin{cases} 0 \quad x <0 \\ 1 \quad x \ge 0 \end{cases}$$ | $[0, +\infty)$ |
|  tanh函数   | $f(x) = \frac{1 - exp(-2x)}{1 + exp(-2x)}$ |                     $f'(x) = 1 - f(x)^2$                     |   $(-1, 1)$    |

下表中的激活函数，需要多个变量作为输入：

|    名称     |                       方程                       |                             导数                             |   值域   |
| :---------: | :----------------------------------------------: | :----------------------------------------------------------: | :------: |
| softmax函数 | $y_i = \frac{\exp(o_i)}{\sum_{j=1}^n \exp(o_j)}$ | $\frac{\partial f_i(\vec{x})}{\partial x_j} = f_i(\vec{x}(\delta_{ij} - f_j(\vec{x})))$ | $(0, 1)$ |

在这里只介绍上面三种，不介绍 *softmax函数*。



### Sigmoid函数

优点：

* 便于求导的平滑函数；
* 能压缩数据，保证数据幅度不会有问题；
* 适合用于前向传播；

缺点：

* 反向传播时，容易出现 **梯度消失（gradient  vanishing）**，从而无法完成 **深层网络** 训练；
* 计算量大，反向传播求误差梯度时，求导涉及除法；
* 收敛缓慢。



*函数* 及其 *导数* 图像：

![Sigmoid](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/Sigmoid.svg "Sigmoid函数图像")

对二维数据的作用效果：

![Sigmoid对二维数据作用效果](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/Sigmoid-data.svg "Sigmoid函数 对二维数据作用效果")

### Tanh函数

优点：

- 解决了 *Sigmoid函数* 输出 *非0均值* 的问题，使其收敛速度要比 *sigmoid函数* 快，减少迭代次数。

缺点：

* 容易出现 **梯度消失（gradient  vanishing）**。



*函数* 及其 *导数* 图像：

![Tanh](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/Tanh.svg "Tanh函数图像")

对二维数据的作用效果：

![Tanh对二维数据作用效果](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/Tanh-data.svg "Tanh函数 对二维数据作用效果")

### ReLU函数

优点：

* 相较于 *sigmoid/tanh* 函数，*ReLU* 对于 SGD 的收敛有巨大的加速作用（[Alex Krizhevsky](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) 指出有 6 倍之多）；
* 运算简单。

缺点：

* 训练的时候很”脆弱”，例如，一个非常大的梯度流过一个 *ReLu* 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都 *dead* 了。



*函数* 及其 *导数* 图像：

![ReLU](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/ReLU.svg "ReLU函数图像")

对二维数据的作用效果：

![ReLU对二维数据作用效果](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-16/ReLU-data.svg "ReLU函数 对二维数据作用效果")



## 4. 多层感知机

*多层感知机* 就是含有至少一个 *隐藏层* 的由全连接层组成的神经网络，且每个隐藏层的输出都通过 *激活函数* 进行变换。在 *多层感知机* 中，其 **层数** 和 各隐藏层中的 **隐藏单元个数** 都是 **超参数**。多层感知机的计算方式如下：


$$
\begin{split}\begin{aligned}
\boldsymbol{H} &= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\\
\boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o,
\end{aligned}\end{split}
$$


其中 $\phi$ 表示 *激活函数*。