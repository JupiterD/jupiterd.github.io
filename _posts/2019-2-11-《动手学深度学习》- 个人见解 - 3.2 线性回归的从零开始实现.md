---
layout: post
title: '《动手学深度学习》- 个人见解 - 3.2 线性回归的从零开始实现'
subtitle: '《动手学深度学习》- 3.2 线性回归的从零开始实现'
description: '《动手学深度学习》个人见解 - 3.2 线性回归的从零开始实现 篇'
date: 2019-02-11
lastmod: 2019-02-11
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# 《动手学深度学习》- 个人见解 - 3.2 线性回归的从零开始实现

直接上代码：

~~~python
from mxnet import autograd, nd


def data_iter(feature, labels, batch_size):
    """获取特征和标签."""
    index = nd.array(range(num_examples))
    # 打乱顺序
    nd.random.shuffle(index)

    for i in range(0, num_examples, batch_size):
        j = index[i:min(i + batch_size, num_examples)]
        yield feature.take(j), labels.take(j)


def linreg(w, x, b):
    """线性回归模型."""
    return nd.dot(x, w) + b


def squared_loss(y_hat, y_true):
    """平方损失函数."""
    return 0.5 * ((y_hat - y_true)**2)


def sgd(params, learning_rate, batch_size):
    """梯度下降算法."""
    for param in params:
        param[:] = param - (learning_rate / batch_size) * param.grad


# 真实 w 和 b
true_w = nd.array([1.2, 6.53])
true_b = 10.3

# 特征个数
num_inputs = 2
# 样本数
num_examples = 1000

# 生成 特征 和 标签
feature = nd.random.normal(loc=2, scale=3.4, shape=(num_examples, num_inputs))
labels = nd.dot(feature, true_w) + true_b
labels += nd.random.normal(scale=0.3, shape=(num_examples))

# 训练次数
num_training = 15
# 批量大小
batch_size = 1

# 初始化 w 和 b
w = nd.random.normal(loc=5, scale=3.4, shape=(num_inputs, 1))
# w = nd.array([1, 6])
b = nd.zeros(shape=(1,))
# 学习速率
learning_rate = 0.001

model = linreg
loss_function = squared_loss
opt = sgd

w.attach_grad()
b.attach_grad()
for i in range(num_training):
    for x, y in data_iter(feature, labels, batch_size):
        with autograd.record():
            loss_values = loss_function(model(w, x, b), y)
        loss_values.backward()
        opt([w, b], learning_rate, batch_size)

    mean_loss = loss_function(model(w, x, b), y).mean().asscalar()
    print("*" * 40)
    print(f"平均损失 -> {mean_loss}")
    print(f"w -> {w.asnumpy().tolist()} -> {list(true_w.asnumpy())}")
    print(f"b -> {list(b.asnumpy())} -> {[true_b]}")

~~~

>****************************************
>平均损失 -> 0.014495118521153927
>
>w -> [[1.6956347227096558], [7.117773532867432]] -> [1.2, 6.53]
>
>b -> [4.908467] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.004121046978980303
>
>w -> [[1.4859787225723267], [6.86623477935791]] -> [1.2, 6.53]
>
>b -> [7.227397] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.0010326007613912225
>
>w -> [[1.3663501739501953], [6.722708225250244]] -> [1.2, 6.53]
>
>b -> [8.550576] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00019118216005153954
>
>w -> [[1.2980915307998657], [6.640809535980225]] -> [1.2, 6.53]
>
>b -> [9.305573] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 1.1478085070848465e-05
>
>w -> [[1.2591428756713867], [6.594080924987793]] -> [1.2, 6.53]
>
>b -> [9.736366] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 6.594229489564896e-06
>
>w -> [[1.2369195222854614], [6.567418098449707]] -> [1.2, 6.53]
>
>b -> [9.982176] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 3.563304926501587e-05
>
>w -> [[1.2242392301559448], [6.552204132080078]] -> [1.2, 6.53]
>
>b -> [10.122426] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 6.254867184907198e-05
>
>w -> [[1.2170039415359497], [6.54352331161499]] -> [1.2, 6.53]
>
>b -> [10.2024555] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 8.126490865834057e-05
>
>w -> [[1.212875485420227], [6.538569927215576]] -> [1.2, 6.53]
>
>b -> [10.248118] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 9.293926996178925e-05
>
>w -> [[1.2105193138122559], [6.5357441902160645]] -> [1.2, 6.53]
>
>b -> [10.274181] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00010014700819738209
>
>w -> [[1.2091758251190186], [6.534130096435547]] -> [1.2, 6.53]
>
>b -> [10.289045] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00010429209214635193
>
>w -> [[1.2084081172943115], [6.533209800720215]] -> [1.2, 6.53]
>
>b -> [10.297535] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00010667456808732823
>
>w -> [[1.2079706192016602], [6.532686233520508]] -> [1.2, 6.53]
>
>b -> [10.302367] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00010812818800332025
>
>w -> [[1.2077219486236572], [6.53238582611084]] -> [1.2, 6.53]
>
>b -> [10.305127] -> [10.3]
>
>
>
>****************************************
>平均损失 -> 0.00010891498095588759
>
>w -> [[1.2075787782669067], [6.532215595245361]] -> [1.2, 6.53]
>
>b -> [10.306703] -> [10.3]