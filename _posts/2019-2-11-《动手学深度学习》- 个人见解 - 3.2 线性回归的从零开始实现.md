---
layout: post
title: '《动手学深度学习》- 个人见解 - 3.2 线性回归的从零开始实现'
subtitle: '《动手学深度学习》- 3.2 线性回归的从零开始实现'
description: '《动手学深度学习》个人见解 - 3.2 线性回归的从零开始实现 篇'
date: 2019-02-11
lastmod: 2019-02-12
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# 《动手学深度学习》- 个人见解 - 3.2 线性回归的从零开始实现

直接上代码：

~~~python
from mxnet import autograd
from mxnet import nd


def data_iter(feature, labels, batch_size):
    """获取特征和标签."""
    index = nd.array(range(num_examples))
    # 打乱顺序
    nd.random.shuffle(index)

    for i in range(0, num_examples, batch_size):
        j = index[i:min(i + batch_size, num_examples)]
        yield feature.take(j), labels.take(j)


def linreg(w, x, b):
    """线性回归模型."""
    return nd.dot(x, w) + b


def squared_loss(y_hat, y_true):
    """平方损失函数."""
    return 0.5 * ((y_hat - y_true)**2)


def sgd(params, learning_rate, batch_size):
    """梯度下降算法."""
    for param in params:
        param[:] = param - (learning_rate / batch_size) * param.grad


# 真实 w 和 b
true_w = nd.array([1.2, 6.53])
true_b = 10.3

# 特征个数
num_inputs = 2
# 样本数
num_examples = 1000

# 生成 特征 和 标签
feature = nd.random.normal(loc=2, scale=3.4, shape=(num_examples, num_inputs))
labels = nd.dot(feature, true_w) + true_b
labels += nd.random.normal(scale=0.3, shape=(num_examples))

# 训练次数
num_training = 15
# 批量大小
batch_size = 1

# 初始化 w 和 b
# w = nd.random.normal(loc=5, scale=3.4, shape=(num_inputs, 1))
w = nd.array([1, 6])
b = nd.zeros(shape=(1, ))
# 学习速率
learning_rate = 0.03

model = linreg
loss_function = squared_loss
opt = sgd

w.attach_grad()
b.attach_grad()
for i in range(num_training):
    for x, y in data_iter(feature, labels, batch_size):
        with autograd.record():
            loss_values = loss_function(model(w, x, b), y)
        loss_values.backward()
        opt([w, b], learning_rate, batch_size)

    mean_loss = loss_function(model(w, feature, b), labels).mean().asscalar()
    print("*" * 40)
    print(f"平均损失 -> {mean_loss}")
    print(f"w -> {w.asnumpy().tolist()} -> {list(true_w.asnumpy())}")
    print(f"b -> {list(b.asnumpy())} -> {[true_b]}")
~~~

>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]
>
>****************************************
>平均损失 -> 0.08129624277353287
>
>w -> [1.161401629447937, 6.4748945236206055] -> [1.2, 6.53]
>
>b -> [10.316691] -> [10.3]