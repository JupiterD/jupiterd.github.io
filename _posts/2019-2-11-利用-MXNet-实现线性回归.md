---
layout: post
title: '利用 MXNet 实现线性回归'
subtitle: '利用 MXNet 实现线性回归'
description: '利用 MXNet 实现线性回归 代码实现'
date: 2019-02-11
lastmod: 2019-02-15
categories: 技术
tags: 深度学习
---
# 利用 MXNet 实现线性回归

## 1. 不利用 Gluon接口

首先利用模型，不利用 MXNet 已有的接口进行实现。

直接上代码：

~~~python
from mxnet import autograd
from mxnet import nd


def data_iter(feature, labels, batch_size):
    """获取特征和标签."""
    index = nd.array(range(num_examples))
    # 打乱顺序
    nd.random.shuffle(index)

    for i in range(0, num_examples, batch_size):
        j = index[i:min(i + batch_size, num_examples)]
        yield feature.take(j), labels.take(j)


def linreg(w, x, b):
    """线性回归模型."""
    return nd.dot(x, w) + b


def squared_loss(y_hat, y_true):
    """平方损失函数."""
    return 0.5 * ((y_hat - y_true)**2)


def sgd(params, learning_rate, batch_size):
    """梯度下降算法."""
    for param in params:
        param[:] = param - (learning_rate / batch_size) * param.grad


# 真实 w 和 b
true_w = nd.array([1.2, 6.53])
true_b = 10.3

# 特征个数
num_inputs = 2
# 样本数
num_examples = 1000

# 生成 特征 和 标签
feature = nd.random.normal(loc=2, scale=3.4, shape=(num_examples, num_inputs))
labels = nd.dot(feature, true_w) + true_b
labels += nd.random.normal(scale=0.3, shape=(num_examples))

# 训练次数
num_training = 3
# 批量大小
batch_size = 1

# 初始化 w 和 b
# w = nd.random.normal(loc=5, scale=3.4, shape=(num_inputs, 1))
w = nd.array([1, 6])
b = nd.zeros(shape=(1, ))
# 学习速率
learning_rate = 0.03

model = linreg
loss_function = squared_loss
opt = sgd

w.attach_grad()
b.attach_grad()
for i in range(num_training):
    for x, y in data_iter(feature, labels, batch_size):
        with autograd.record():
            loss_values = loss_function(model(w, x, b), y)
        loss_values.backward()
        opt([w, b], learning_rate, batch_size)

    mean_loss = loss_function(model(w, feature, b), labels).mean().asscalar()
    print("*" * 40)
    print(f"平均损失 -> {mean_loss}")
    print(f"w -> {w.asnumpy().tolist()} -> {list(true_w.asnumpy())}")
    print(f"b -> {list(b.asnumpy())} -> {[true_b]}")
~~~

>****************************************
>平均损失 -> 0.046825848519802094
>
>w -> [1.2128626108169556, 6.540095329284668] -> [1.2, 6.53]
>
>b -> [10.260407] -> [10.3]
>
>****************************************
>平均损失 -> 0.04682578146457672
>
>w -> [1.2128621339797974, 6.540095329284668] -> [1.2, 6.53]
>
>b -> [10.260408] -> [10.3]
>
>****************************************
>平均损失 -> 0.04682578146457672
>
>w -> [1.2128621339797974, 6.540095329284668] -> [1.2, 6.53]
>
>b -> [10.260408] -> [10.3]



## 2. 利用 Gluon接口

接下来利用 MXNet 接口实现线性回归。

下图是各包的调用关系：

![线性回归的实现 - 调用关系 - 用途](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-12/zh-d2l-3.3.png "线性回归的实现 - 调用关系 - 用途")

~~~python
from mxnet import autograd
from mxnet import gluon
from mxnet import init
from mxnet import nd
from mxnet.gluon import data as gdata
from mxnet.gluon import loss as gloss
from mxnet.gluon import nn

# 真实 w 和 b
true_w = nd.array([1.2, 6.53])
true_b = 10.3

# 特征个数
num_inputs = 2
# 样本数
num_examples = 1000

# 生成 特征 和 标签
feature = nd.random.normal(loc=2, scale=3.4, shape=(num_examples, num_inputs))
labels = nd.dot(feature, true_w) + true_b
labels += nd.random.normal(scale=0.3, shape=(num_examples))

# 读取数据
batch_size = 10
dataset = gdata.ArrayDataset(feature, labels)
data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)

# 生成神经网络
net = nn.Sequential()
net.add(nn.Dense(1))

# 初始化
net.initialize(init.Normal())
loss = gloss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})

num_training = 3
for i in range(num_training):
    for x, y in data_iter:
        with autograd.record():
            l = loss(net(x), y)
        l.backward()
        trainer.step(batch_size)
    l = loss(net(feature), labels)
    dense = net[0]
    w = dense.weight.data()
    b = dense.bias.data()
    print('*' * 40)
    print(f'loss -> {l.mean().asnumpy()}')
    print(f'w -> {w.asnumpy()} -> {true_w.asnumpy()}')
    print(f'b -> {b.asnumpy()} -> {true_b}')
~~~

>****************************************
>loss -> [0.87920487]
>
>w -> [[1.3497797 6.5853043]] -> [1.2  6.53]
>
>b -> [8.714178] -> 10.3
>
>****************************************
>loss -> [0.06646211]
>
>w -> [[1.221841 6.561642]] -> [1.2  6.53]
>
>b -> [10.03347] -> 10.3
>
>****************************************
>loss -> [0.04556826]
>
>w -> [[1.2125463 6.5216722]] -> [1.2  6.53]
>
>b -> [10.257837] -> 10.3