---
layout: post
title: '《动手学深度学习》- 个人见解 - 3.3 线性回归的简洁实现'
subtitle: '《动手学深度学习》- 3.3 线性回归的简洁实现'
description: '《动手学深度学习》个人见解 - 3.3 线性回归的简洁实现 篇'
date: 2019-02-12
lastmod: 2019-02-12
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# 《动手学深度学习》- 个人见解 - 3.3 线性回归的简洁实现

本章中各包 **调用关系** 和 **用途** 表：

![3.3 线性回归的简洁实现 - 调用关系 - 用途](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-12/zh-d2l-3.3.png)

直接上代码：

~~~python
from mxnet import autograd
from mxnet import gluon
from mxnet import init
from mxnet import nd
from mxnet.gluon import data as gdata
from mxnet.gluon import loss as gloss
from mxnet.gluon import nn

# 真实 w 和 b
true_w = nd.array([1.2, 6.53])
true_b = 10.3

# 特征个数
num_inputs = 2
# 样本数
num_examples = 1000

# 生成 特征 和 标签
feature = nd.random.normal(loc=2, scale=3.4, shape=(num_examples, num_inputs))
labels = nd.dot(feature, true_w) + true_b
labels += nd.random.normal(scale=0.3, shape=(num_examples))

# 读取数据
batch_size = 10
dataset = gdata.ArrayDataset(feature, labels)
data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)

# 生成神经网络
net = nn.Sequential()
net.add(nn.Dense(1))

# 初始化
net.initialize(init.Normal())
loss = gloss.L2Loss()
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})

num_training = 15
for i in range(num_training):
    for x, y in data_iter:
        with autograd.record():
            l = loss(net(x), y)
        l.backward()
        trainer.step(batch_size)
    l = loss(net(feature), labels)
    dense = net[0]
    w = dense.weight.data()
    b = dense.bias.data()
    print('*' * 40)
    print(f'loss -> {l.mean().asnumpy()}')
    print(f'w -> {w.asnumpy()} -> {true_w.asnumpy()}')
    print(f'b -> {b.asnumpy()} -> {true_b}')
~~~

>****************************************
>loss -> [0.7155371]
>
>w -> [[1.321642  6.7106915]] -> [1.2  6.53]
>
>b -> [8.826525] -> 10.3
>
>****************************************
>loss -> [0.07426619]
>
>w -> [[1.1981442 6.5371246]] -> [1.2  6.53]
>
>b -> [10.042526] -> 10.3
>
>****************************************
>loss -> [0.046647]
>
>w -> [[1.201956 6.518797]] -> [1.2  6.53]
>
>b -> [10.255667] -> 10.3
>
>****************************************
>loss -> [0.04551357]
>
>w -> [[1.1936108 6.5191655]] -> [1.2  6.53]
>
>b -> [10.294672] -> 10.3
>
>****************************************
>loss -> [0.04383859]
>
>w -> [[1.2002493 6.5270014]] -> [1.2  6.53]
>
>b -> [10.29326] -> 10.3
>
>****************************************
>loss -> [0.04599086]
>
>w -> [[1.20824  6.543589]] -> [1.2  6.53]
>
>b -> [10.30156] -> 10.3
>
>****************************************
>loss -> [0.04487025]
>
>w -> [[1.211516 6.523223]] -> [1.2  6.53]
>
>b -> [10.297635] -> 10.3
>
>****************************************
>loss -> [0.0444792]
>
>w -> [[1.2046136 6.5383673]] -> [1.2  6.53]
>
>b -> [10.302191] -> 10.3
>
>****************************************
>loss -> [0.04411273]
>
>w -> [[1.2058469 6.5252004]] -> [1.2  6.53]
>
>b -> [10.29427] -> 10.3
>
>****************************************
>loss -> [0.0441874]
>
>w -> [[1.1966225 6.5403633]] -> [1.2  6.53]
>
>b -> [10.294469] -> 10.3
>
>****************************************
>loss -> [0.04824059]
>
>w -> [[1.2192925 6.5111303]] -> [1.2  6.53]
>
>b -> [10.297476] -> 10.3
>
>****************************************
>loss -> [0.0468571]
>
>w -> [[1.1960636 6.552139 ]] -> [1.2  6.53]
>
>b -> [10.298216] -> 10.3
>
>****************************************
>loss -> [0.04600855]
>
>w -> [[1.1838665 6.5447054]] -> [1.2  6.53]
>
>b -> [10.292118] -> 10.3
>
>****************************************
>loss -> [0.04582029]
>
>w -> [[1.189101 6.547934]] -> [1.2  6.53]
>
>b -> [10.291564] -> 10.3
>
>****************************************
>loss -> [0.04711237]
>
>w -> [[1.2208458 6.5205107]] -> [1.2  6.53]
>
>b -> [10.300468] -> 10.3