---
layout: post
title: 'softmax回归 介绍'
subtitle: 'softmax回归 介绍'
description: '深入介绍了 softmax回归'
date: 2019-02-14
lastmod: 2019-02-14
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# softmax回归 介绍

之前介绍的 **线性回归** 适用于输出为 *连续值* 的场景。而在另一种场景中，输出值将从连续值变为 **离散值**。在离散值中，基本的预测模型有两种：**logistic回归** 和 **softmax回归** ，但是 *logistic回归* 只能用于对两种标签进行分类，*softmax回归* 则可以对多种标签的数据进行分类，也就是说 *softmax回归* 是*logistic回归* 的扩充。



## 1. 问题引入

现在假设有一副长宽均为 2 个像素的图像，且色彩均为灰度。我们将图像中这 4 个像素分别用 $x_1, x_2, x_3, x_4$ 表示，并且训练集中的图像的真实标签有猫，狗，兔三类动物，这些标签分别用 $y_1, y_2, y_3$ 表示，并且令 $y_1 = 1, y_2 = 2, y_3 = 3$，此时我们可以分别利用 1, 2, 3 来表示猫，狗，兔。



## 2. 利用 softmax回归 建立模型

*softmax回归* 与 *线性回归* 从某种角度来说建立方式类似，我们可以很自然地想到分别利用三个线性关系来表达三个标签，即：


$$
o_1 = \omega_{11}x_1 + \omega_{12}x_2 + \omega_{13}x_3 + \omega_{14}x_4 + b_1
\\
o_2 = \omega_{21}x_1 + \omega_{22}x_2 + \omega_{23}x_3 + \omega_{24}x_4 + b_2
\\
o_3 = \omega_{31}x_1 + \omega_{32}x_3 + \omega_{33}z_3 + \omega_{34}x_4 + b_3
$$

其中 $o_i$ 表示输出的三种标签的值，$w_{ij}$ 表示第 $j$ 个像素在第 $i$ 个标签中的权重，$b_i$ 表示偏差。

当 $o_i$ 最大时，则表示该图像为第 $i​$ 种动物。

![softmax回归](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-14/softmax-o-compression.jpg "softmax回归")

从其表达式可以看出， *softmax回归* 的神经网络表示与 *线性回归* 十分类似，只是输入和输出变多了，而其他则完全相同。

![softmax 神经网络](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-13/softmaxreg.svg "softmax 神经网络示意图")

通过上述的表达式，我们可以看出，此时 *softmax回归* 的输出值 **范围不确定**、**难以衡量误差** 并且 **难以解释**。此时，我们就需要引入 **softmax函数** 来对输出值进行转换，使其输出值能够方便我们处理。

### 2.1 引入 softmax运算

**softmax运算** 是专门用来解决 *softmax回归* 的输出值难以处理的问题的。

*softmax运算* 引入的 **softmax运算符（softmax operator）** 将输出值进行非线性归一化，即：


$$
\hat{y_1}, \hat{y_2}, \hat{h_3} = softmax(o_1, o_2, o_3)
$$


其中：


$$
\hat{y_i} = \frac{\exp(o_i)}{\sum_{j=1}^n \exp(o_j)}
$$

这就是 *softmax函数*。

通过这种方法，我们可以自然地将输出值变为一个合法的 *概率分布*。假设此时 $\hat{y_1} = 0.7$，我们就可以很清楚的知道此图像为猫的概率为 $70\%$，这样的一个数据总比 $o_1 = 30$ 来的更有意义吧。

![softmax函数](http://jupiterd-top-image.oss-cn-hangzhou.aliyuncs.com/19-2-14/softmax-y-compression.jpg "利用softmax函数运算")

至于在这里为什么不使用标准化，而是使用这种奇怪的方式，这里有两种观点来解释这个问题 [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/linear-classify/#softmax)：

1. 信息论观点：*softmax函数* 可以看作是试图最小化预测值和真实值之间的 *交叉熵*。
2. 概率论观点：我们实际上是在观察对数概率，因此，当我们进行幂运算时，我们最终得到的是原始概率。在这种情况下，*softmax函数* 找到了 *最大似然估计(MLE)*。



那么，此时对于 *softmax回归*  的计算表达式可以表示为如下所示：

令图像中各像素为：


$$
\vec{x}^{(i)} = \begin{bmatrix} x_1^{(i)}, x_2^{(i)}, x_3^{(i)}, x_4^{(i)} \end{bmatrix}
$$


令权重和偏差为：


$$
\vec{w} = \begin{bmatrix}
\omega_{11} \quad \omega_{12} \quad \omega_{13} \quad \omega_{14} \\
\omega_{21} \quad \omega_{22} \quad \omega_{23} \quad \omega_{24} \\
\omega_{31} \quad \omega_{32} \quad \omega_{33} \quad \omega_{34} \\
\end{bmatrix}
,\quad \vec{b} = \begin{bmatrix} b_1, b_2, b_3 \end{bmatrix}
$$


输出层值为：


$$
\vec{o}^{(i)} = \begin{bmatrix} o_1^{(i)}, o_2^{(i)}, o_3^{(i)} \end{bmatrix}
$$


预测值的概率分布为：


$$
\boldsymbol{\hat{y}^{(i)}} = \begin{bmatrix} y_1^{(i)}, y_2^{(i)}, y_3^{(i)} \end{bmatrix}
$$


那么其向量表达式为：


$$
\vec{o}^{(i)} = \vec{w}^\mathrm{T}x^{(i)} + \vec{b}
\\
\boldsymbol{\hat{y}^{(i)}} = softmax(\vec{o}^{(i)})
$$


## 3. 交叉熵 损失函数介绍

对于 *softmax回归* 合适的损失函数是 **交叉熵（cross-entropy）**，相对于 *线性回归* 中的 *平方损失*，*交叉熵* 可以避免对数值大小过于严格的问题。

交叉熵的表达式为：


$$
\boldsymbol{H(y^{(i)}, \hat{y}^{(i)})} = -\sum_{j=1}^n y_j^{(i)} \log(\hat{y}_j^{(i)})
$$

其中：


$$
y_j^{(i)} = \begin{cases}
1, \quad  图像标签为第j种
\\
0, \quad 其他
\end{cases}
$$


通过上式，我们可以看出 *交叉熵* 只关注预测正确时的概率，因为在预测错误时 $y_j^{(i)} = 0$，因此，当图像只能是标签中的某一种时，我们可以将其化简为：


$$
\boldsymbol{H(y^{(i)}, \hat{y}^{(i)})} = -\log(\hat{y}_{y^{(i)}}^{(i)})
$$


其中 $y^{(i)}​$ 表示图像正确标签的所对应的值。

对于训练批量数据集的 *交叉熵* 损失函数与 *平方损失* 类似，即：

$$
\ell(\Theta) = \frac{1}{n} \sum_{i=1}^n \boldsymbol{H(y^{(i)}, \hat{y}^{(i)})}
$$
