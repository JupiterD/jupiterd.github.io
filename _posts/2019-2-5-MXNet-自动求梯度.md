---
layout: post
title: 'MXNet 自动求梯度'
subtitle: 'MXNet 自动求梯度'
description: 'MXNet 自动求梯度 篇'
date: 2019-02-05
lastmod: 2019-02-13
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# MXNet 自动求梯度

利用 MXNet 的 `NDArray` 和 `autograd` 模块，可以很轻松的对函数进行求导。



## 1. 简单例子

例如，我们对以下例子进行求导：


$$
\vec{x} = \begin{bmatrix} 0 \\ 1 \\ 2 \\ 3 \end{bmatrix}
\\
y = 2 \vec{x}^T \vec{x}
$$


根据上式，我们可以得到 `y` 关于 `x` 的梯度，即：


$$
\frac{dy}{dx} = 4 \vec{x} = \begin{bmatrix} 0 \\ 4 \\ 8 \\ 12 \end{bmatrix}
$$


我们用代码验证一下：

~~~python
from mxnet import autograd, nd

x = nd.arange(4).reshape((4, 1))
x.attach_grad()

with autograd.record():
    y = 2 * nd.dot(x.T, x)

y.backward()

x.grad
~~~

>[[ 0.]
>
> [ 4.]
> 
> [ 8.]
> 
> [12.]]
> 
><NDArray 4x1 @cpu(0)>

从上面的例子中我们可以发现，当我们需要计算多个变量的梯度时，只要运行 *对应变量的 `.grad`* 即可。



## 2. 对控制流求梯度

这里，我们将要实现一个函数用来模拟以下 `f(x)`：


$$
f(x) = \begin{cases}
\vec{x}, \quad \sum_{i=0}^n x_i \leq 10
\\\\
2\vec{x}, \quad other
\end{cases}
$$


~~~python
def f(x):
    if x.sum().asscalar() <= 10:
        return x
    
    return 2 * x
~~~

验证代码：

~~~python
x = nd.arange(4).reshape((4, 1))
x.attach_grad()

with autograd.record():
    y = f(x)

y.backward()

x.grad
~~~

>[[1.]
>
>[1.]
>
>[1.]
>
>[1.]]
>
><NDArray 4x1 @cpu(0)>



## 3. 预测模式与训练模式

MXNet 在使用了 `autograd.record()` 之后会将 **默认** 的 **预测模式** 转换为 **训练模式**。

| autograd.record() |   模式   |
| :---------------: | :------: |
|   关闭（默认）    | 预测模式 |
|       开启        | 训练模式 |

这可以通过 `autograd.is_training()` 函数来查看。

~~~python
print(autograd.is_training())

with autograd.record():
    print(autograd.is_training())
~~~

>False
>
>True

需要注意的是，在有些情况下，同一个模型在 *训练模式* 与 *预测模式* 的 **行为不相同**。