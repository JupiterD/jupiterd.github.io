---
layout: post
title: '《动手学深度学习》- 个人见解 - 3.1 线性回归'
subtitle: '《动手学深度学习》- 3.1 线性回归'
description: '《动手学深度学习》个人见解 - 3.1 线性回归 篇'
date: 2019-02-08
lastmod: 2019-02-08
categories: 技术
tags: 《动手学深度学习》 机器学习
---
# 《动手学深度学习》- 个人见解 - 3.1 线性回归

*本文将通过 **线性回归模型** 来介绍 **深度学习** 的基本 **概念** 与 **技术***。

## 1. 深度学习的基本要素

**线性回归** 是机器学习里的一个基本模型，在这里介绍 *线性回归* 的原因是 *线性回归* 是 **单层神经网络**，因此其涉及到的 *概念和技术* 同样适用于大多数的 *深度学习模型*。

其基本要素分为以下三类：

1. **构建模型**
2. **模型训练**
3. **模型预测**



## 2. 构建模型

**构建模型** 指的就是要对问题进行抽象，利用 *数学方法* 和 *数据（问题）本身* 的结构，构建一个能够解决问题的模型。



### 2.1 线性回归介绍

为了便于接下去的理解，先来说说 **线性回归** 的定义。



**定义**：对于给定 **数据集** $D = \{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(i)}, y^{(i)})\}$ 其中 $x^{(i)} = (x_{i1}, x_{i2}, \cdots, x_{in})$，我们试图利用数据集找出一个 **线性模型**，使其能够反映出 $x^{(i)}$ 和 $y^{(i)}$ 之间的关系。这里的 *线性模型* 可以表示为：


$$
f(x) = \omega_1x_1 + \omega_2x_2 + ... + \omega_nx_n + b
$$


通常我们将其公式写成如下所示的 **向量** 形式：


$$
f(x) = \vec{\omega}^T\vec{x} + b
$$


其中：


$$
\vec{\omega} = \begin{bmatrix}\omega_1\\ \omega_2\\ \vdots\\ \omega_n\end{bmatrix}, \vec{x} = \begin{bmatrix}x_1\\ x_2\\ \vdots\\ x_n\end{bmatrix}
$$


**解释**：这里的 $\vec{\omega}​$ 的意思是 **weight**，即 **权重**，表示对应的 **属性** 的 **重要程度**，这很好理解，*属性* 越重要，其对结果的影响越大。$\vec{x}​$ 中的 $x_n​$ 就是对应的 *属性* 的 **数据**。式子中的 $b​$ 的意思是 **bias**，即 **偏差**，通常用于修正模型的 **误差**。



### 2.2 房价预测模型构建

假设我们现在需要利用房地产数据预测房价，并且我们假设房价只与 *面积* 和 *房龄* 有关。

我们令 *面积* 数据为 $x_1$，权重为 $\omega_1$，*房龄* 数据为 $x_2$，权重为 $\omega_2$，那么其 *线性回归模型* 的表达式为：


$$
\hat{y} = \omega_1 x_1 + \omega_2 x_2 + b
$$


这里的 $\hat{y}$ 指的是预测的 *房价*。



## 3. 模型训练

**模型训练** 是 *深度学习* 的一个重要环节，*模型训练* 的好坏，直接影响到最终 *模型预测* 的结果好坏，即便是 *模型构建* 的很好，但是 *模型训练* 的不好，也很难体现出模型的优势。

在 *模型训练* 环节，有 3 个重要的东西需要确定，即：

1. **训练数据**
2. **损失函数**
3. **优化算法**



### 3.1 训练数据

我们通常利用所给的 **训练集（training set）** 训练模型，试图从中计算出合适的 **参数** 使得模型的 *预测结果* 与 *真实结果* 误差最小。在本例中，每一个房屋即为一个 **样本（sample）**，每个房屋真实的出售价格即为一个 **标签（label）**，用来预测标签的两个因素 *面积（$\omega_1$）* 和 *房龄（$\omega_2$）* 即为两个 **特征（feature）**。特征用来表示样本的 **特点**。

在本例中，假设我们共有 $n$ 个样本，对于第 $i$ 个样本的 *特征* 为 $x^{(i)}_1$ 和 $x^{(i)}_2$，*标签* 为 $y^{(i)}$。那么对于第 $i$ 个样本的表示即为：


$$
\hat{y}^{(i)} = \omega_1 x^{(i)}_1 + \omega_2 x^{(i)}_2 + b, \quad i\in[1, n]
$$


这里的 $\hat{y}^{(i)}$ 指的是 *标签* 中的数据。



### 3.2 损失函数

在模型训练中，我们需要知道在当前 *参数* 情况下，*预测值* 与 *真实值* 之间的 **误差** 大小，此时我们就需要采用一个合适的 **损失函数** 来衡量。不同的 *损失函数* 具有不同的 *性能* 和 *精度*，因此，选取一个合适的 *损失函数* 是十分重要的。

一种常用的 *损失函数* 是 **平方函数**。本例中，它在评估第 $i$ 个样本的误差的表达式为：


$$
\ell^{(i)}(\omega_1, \omega_2, b) = \frac{1}{2}(\hat{y}^{(i)} - y^{(i)})^2
$$


通常，我们会使用 *平均误差* 来衡量预测的质量，即：


$$
\ell(\omega_1, \omega_2, b) = \frac{1}{n} \sum_{i=1}^n \ell^{(i)}(\omega_1, \omega_2, b)
$$



### 3.3 优化算法

再具体了解 *优化算法* 之前，我们需要先了解 *深度学习* 中两种解的区别，即：

**解析解（analytical solution）**：指能利用 **公式** 直接解出参数的最优解的方法，但其前提条件是 *模型* 和 *损失函数* 比较简单，因此大多数模型都无法利用 *解析解* 解出。

**数值解（numerical solution）**：指利用 *一系列特定的值逐步逼近真实解* 的方法。

例如解如下方程：


$$
x^2 = 2
$$


*解析解*：$x = \pm \sqrt{2}$

*数值解*：$x = 1.4142​$



#### 3.3.1 小批量随机梯度下降

**小批量随机梯度下降（mini-batch stochastic gradient descent）** 在 *深度学习* 中被广泛使用。其算法是：首先随机初始化 *参数* 的初始值，随后利用数据不断迭代，保证在每次迭代后 *损失函数* 的值都会有所降低。在每次迭代中，都会 *随机采样由固定数量的样本组成的 **小批量（mini-batch）**$\mathcal{B}​$*，然后计算 *小批量* 中数据样本的 *损失函数* 有关 *模型参数* 的 **导数（梯度）**，最后用此 *导数（梯度）* 与一个预先设置好的 *正数* 的乘积作为 *模型参数* 在本次迭代中的 *减小量*。

此方法与类似的梯度下降算法有一个共同的问题，即 *当参数分布存在 **多峰** 时，其解通常为 **局部最优解***。

在本例中，每个 *参数* 迭代时的计算方式如下：


$$
\omega_1 \gets \omega_1 - \frac{\eta}{\vert \mathcal{B} \vert}\sum_{i \in \mathcal{B}} \frac{\partial \ell^{(i)}(\omega_1, \omega_2, b)}{\partial \omega_1}
\\
\omega_2 \gets \omega_2 - \frac{\eta}{\vert \mathcal{B} \vert}\sum_{i \in \mathcal{B}} \frac{\partial \ell^{(i)}(\omega_1, \omega_2, b)}{\partial \omega_2}
\\
b \gets b - \frac{\eta}{\vert \mathcal{B} \vert}\sum_{i \in \mathcal{B}} \frac{\partial \ell^{(i)}(\omega_1, \omega_2, b)}{\partial b}
$$


其中 $\vert \mathcal{B} \vert$ 指的是 *小批量* 中的 **样本数量（批量大小，batch size）**，$\eta$ 指的是 **学习速率（learning rate）**，通常取 **正数**。



## 3.4 模型预测

模型预测指的就是利用训练完的 *参数*，使用模型进行估计。